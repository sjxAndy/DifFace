name: deblur_gopro
phase: train

path:
  log: logs
  tb_logger: tb_logger
  results: results
  checkpoint: checkpoint
  resume_state: /mnt/lustre/sunjixiang1/code/DifFace/logs/07111602/ckpts/model_310000
  # resume_state: experiments/sr_ffhq_210806_204158/checkpoint/I640000_E37 # pretrain model or training state


data:
  train:
    name: gopro-train
    type: lqhqnor
    params:
      phase: train
      sdk_gt: s3://Deblur/GoPro/crop/sharp_crops
      sdk_lq: s3://Deblur/GoPro/crop/blur_crops
      meta_info_file: s3://Deblur/GoPro/crop/blur_crops.lmdb/meta_info.txt
      filename_tmpl: '{}'
      io_backend:
        type: lmdb

      gt_size: 128
      use_flip: true
      use_rot: true
      # data loader
      use_shuffle: true
      num_worker_per_gpu: 2
      batch_size_per_gpu: 1
      dataset_enlarge_ratio: 100
      prefetch_mode: ~
      scale: 1
  val:
    name: gopro-test
    type: lqhqnor
    params:
      phase: val
      # dataroot_gt: /mnt/lustre/leifei1/data/deblur/test/target.lmdb
      # dataroot_lq: /mnt/lustre/leifei1/data/deblur/test/input.lmdb
      sdk_gt: s3://Deblur/GoPro/crop/sharp_crops
      sdk_lq: s3://Deblur/GoPro/crop/blur_crops
      meta_info_file: s3://Deblur/GoPro/test/input.lmdb/meta_info.txt
      io_backend:
        type: lmdb
      scale: 1

model:
  params:
    which_model_G: deblurdiffusion
    finetune_norm: false
    unet:
      in_channel: 6
      out_channel: 3
      inner_channel: 32
      channel_multiplier: [1, 2, 4, 8, 8]
      attn_res: [16]
      res_blocks: 2
      dropout: 0.2
    beta_schedule:
      train:
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-6
        linear_end: 1e-2
      val:
        schedule: linear
        n_timestep: 100
        linear_start: 1e-6
        linear_end: 1e-2
    diffusion:
      image_size: 128
      channels: 3
      conditional: true

train:
  # n_iter: 1000000
  # val_freq: 1e4
  save_checkpoint_freq: 1e4
  print_freq: 200
  optimizer:
    type: adam
    lr: 1e-4

  lr: 1e-4
  batch: [32, 4]   # batchsize for training and validation
  microbatch: 4
  use_fp16: False
  num_workers: 16
  prefetch_factor: 2
  iterations: 1600000
  weight_decay: 0
  scheduler: step   # step or cosin
  milestones: [10000, 1600000]
  ema_rates: [0.999]
  save_freq: 10000
  val_freq: 500000000
  log_freq: [1000, 2000]